{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467d6df4-3b12-4cb6-b8e4-bd15fa2354c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing batch inference of Music Flamingo\n",
    "from transformers import (\n",
    "    MusicFlamingoForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    TextStreamer\n",
    ")\n",
    "import torch\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a8131b-e458-45e8-b47c-10f5b4be659d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689bcf9756a14772a74367ee59516331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1027 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MusicFlamingoForConditionalGeneration.from_pretrained(\n",
    "    \"./music_flamingo_fp8\",\n",
    "    device_map=\"cuda\",\n",
    "    dtype=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"./music_flamingo_fp8\")\n",
    "streamer = TextStreamer(processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383f3b6f-7c45-45e0-9175-de860a79db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perf flags\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Enable KV cache (default config has use_cache=False)\n",
    "for cfg in (model.config, model.language_model.config, getattr(model.config, \"text_config\", None)):\n",
    "    if cfg is not None and hasattr(cfg, \"use_cache\"):\n",
    "        cfg.use_cache = True\n",
    "model.generation_config.use_cache = True\n",
    "model.language_model.generation_config.use_cache = True\n",
    "model.generation_config.cache_implementation = \"dynamic\"\n",
    "model.language_model.generation_config.cache_implementation = \"dynamic\"\n",
    "model.generation_config.max_new_tokens = 2048\n",
    "model.generation_config.do_sample = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b82b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-TPS batched inference using SGLang's low-level runner\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "SGLANG_PYTHON_PATH = os.path.join(os.getcwd(), \"sglang\", \"python\")\n",
    "if SGLANG_PYTHON_PATH not in sys.path:\n",
    "    sys.path.insert(0, SGLANG_PYTHON_PATH)\n",
    "\n",
    "_sglang_runner = None\n",
    "\n",
    "\n",
    "def _get_sglang_runner(attention_backend: str = \"triton\", tp_size: int = 1):\n",
    "    global _sglang_runner\n",
    "    if _sglang_runner is not None:\n",
    "        return _sglang_runner\n",
    "\n",
    "    from sglang.bench_one_batch import load_model\n",
    "    from sglang.srt.entrypoints.engine import _set_envs_and_config\n",
    "    from sglang.srt.layers.moe import initialize_moe_config\n",
    "    from sglang.srt.layers.quantization.fp8_utils import initialize_fp8_gemm_config\n",
    "    from sglang.srt.server_args import PortArgs, ServerArgs\n",
    "\n",
    "    model_override = {\"architectures\": [\"MusicFlamingoQwen2ForCausalLM\"], \"model_type\": \"qwen2\"}\n",
    "    server_args_kwargs = {\n",
    "        \"model_path\": \"./music_flamingo_fp8\",\n",
    "        \"tokenizer_path\": \"./music_flamingo_fp8\",\n",
    "        \"trust_remote_code\": True,\n",
    "        \"tp_size\": tp_size,\n",
    "        \"disable_radix_cache\": True,\n",
    "        \"json_model_override_args\": json.dumps(model_override),\n",
    "        \"fp8_gemm_runner_backend\": \"cutlass\",\n",
    "        \"disable_cuda_graph\": True,\n",
    "        \"sampling_backend\": \"pytorch\",\n",
    "        \"grammar_backend\": \"none\",\n",
    "    }\n",
    "    if attention_backend:\n",
    "        server_args_kwargs[\"attention_backend\"] = attention_backend\n",
    "\n",
    "    server_args = ServerArgs(**server_args_kwargs)\n",
    "    _set_envs_and_config(server_args)\n",
    "    initialize_moe_config(server_args)\n",
    "    initialize_fp8_gemm_config(server_args)\n",
    "\n",
    "    port_args = PortArgs.init_new(server_args)\n",
    "    model_runner, _ = load_model(server_args, port_args, gpu_id=0, tp_rank=0)\n",
    "    _sglang_runner = model_runner\n",
    "    return _sglang_runner\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def infer_music_batch(\n",
    "    batch: List[Dict[str, Any]],\n",
    "    max_new_tokens: int = 256,\n",
    "    attention_backend: str = \"triton\",\n",
    "):\n",
    "    \"\"\"\n",
    "    batch: list of {\"text\": str, \"audio\": path-or-array}\n",
    "    returns: (music_embeds_list, descriptions_list)\n",
    "    \"\"\"\n",
    "    # Build batched prompts + audio\n",
    "    conversations = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": item[\"text\"]},\n",
    "                    {\"type\": \"audio\", \"path\": \"<audio>\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        for item in batch\n",
    "    ]\n",
    "    prompts = processor.apply_chat_template(\n",
    "        conversations,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    audios = []\n",
    "    for item in batch:\n",
    "        audio = item[\"audio\"]\n",
    "        if isinstance(audio, str):\n",
    "            audio, _ = librosa.load(audio, sr=16000)\n",
    "        audios.append(audio)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        audio=audios,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    input_features = inputs[\"input_features\"]\n",
    "    input_features_mask = inputs[\"input_features_mask\"]\n",
    "    audio_times = inputs.get(\"audio_times\")\n",
    "\n",
    "    # Audio tower -> projector (keep per-sample embeddings)\n",
    "    encoder_out = model.audio_tower(\n",
    "        input_features,\n",
    "        input_features_mask=input_features_mask,\n",
    "        audio_times=audio_times,\n",
    "    )\n",
    "    audio_embeds_full = model.multi_modal_projector(encoder_out.last_hidden_state)\n",
    "\n",
    "    post_lengths = (input_features_mask.sum(-1) - 2) // 2 + 1\n",
    "    valid_mask = (\n",
    "        torch.arange(audio_embeds_full.shape[1], device=post_lengths.device)[None, :]\n",
    "        < post_lengths[:, None]\n",
    "    )\n",
    "    audio_embeds_flat = audio_embeds_full[valid_mask]\n",
    "\n",
    "    inputs_embeds = model.get_input_embeddings()(input_ids)\n",
    "    audio_token_mask = (input_ids == model.config.audio_token_id).unsqueeze(-1)\n",
    "    inputs_embeds = inputs_embeds.masked_scatter(audio_token_mask, audio_embeds_flat)\n",
    "\n",
    "    music_embeds = [\n",
    "        audio_embeds_full[i, : int(post_lengths[i])].detach().cpu()\n",
    "        for i in range(audio_embeds_full.size(0))\n",
    "    ]\n",
    "\n",
    "    # SGLang batched decode (same high-TPS path as bench_inference)\n",
    "    from sglang.bench_one_batch import decode, extend\n",
    "    from sglang.srt.managers.schedule_batch import Req\n",
    "    from sglang.srt.sampling.sampling_params import SamplingParams\n",
    "\n",
    "    runner = _get_sglang_runner(attention_backend=attention_backend)\n",
    "    sampling_params = SamplingParams(temperature=0.0, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    input_embeds_list = inputs_embeds.detach().cpu().float().tolist()\n",
    "    reqs = []\n",
    "    for i, emb in enumerate(input_embeds_list):\n",
    "        fake_ids = [1] * len(emb)\n",
    "        req = Req(\n",
    "            rid=str(i),\n",
    "            origin_input_text=\"\",\n",
    "            origin_input_ids=fake_ids,\n",
    "            sampling_params=sampling_params,\n",
    "            input_embeds=emb,\n",
    "        )\n",
    "        req.fill_ids = req.origin_input_ids\n",
    "        req.extend_input_len = len(req.fill_ids) - len(req.prefix_indices)\n",
    "        req.logprob_start_len = len(req.origin_input_ids) - 1\n",
    "        reqs.append(req)\n",
    "\n",
    "    next_token_ids, _, batch_state = extend(reqs, runner)\n",
    "    if hasattr(next_token_ids, \"tolist\"):\n",
    "        next_token_ids = next_token_ids.tolist()\n",
    "\n",
    "    output_ids = [[int(tok)] for tok in next_token_ids]\n",
    "    for _ in range(max_new_tokens - 1):\n",
    "        next_token_ids, _ = decode(next_token_ids, batch_state, runner)\n",
    "        if hasattr(next_token_ids, \"tolist\"):\n",
    "            next_token_ids = next_token_ids.tolist()\n",
    "        for idx, tok in enumerate(next_token_ids):\n",
    "            output_ids[idx].append(int(tok))\n",
    "\n",
    "    descriptions = processor.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return music_embeds, descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb49fe8-14fd-49f8-8645-e541d7c42091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eeeaa0c-437c-4a23-8d89-fe740a33f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this track in full detail - tell me the genre, tempo, and key, then dive into the instruments, production style, and overall mood it creates.\"},\n",
    "            {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/song_1.mp3\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Build prompt without loading audio inside apply_chat_template\n",
    "prompt = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "audio, _ = librosa.load(\n",
    "    \"../music/Lorde-Pure_Heroine-24BIT-WEB-FLAC-2013-TVRf/04-lorde-ribs.flac\",\n",
    "    sr=16000,\n",
    ")\n",
    "inputs = processor(\n",
    "    text=prompt,\n",
    "    audio=audio,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd1bbef-5000-49a2-bee5-6a646b8aa74f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#warmup\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m.get_audio_features(**inputs)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#warmup\n",
    "model.generate(**inputs, max_new_tokens=2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35515418-5690-4066-b6ba-7862be2901c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.17148250660933"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "outputs = model.generate(**inputs, max_new_tokens=2056)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "new_tokens = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "toks_per_s = new_tokens / elapsed\n",
    "toks_per_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f240619-51bb-43cb-88d9-1852b5bd065d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.17148250660933"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_per_s\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navidrome",
   "language": "python",
   "name": "nav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
