{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3397d476-e231-48f4-8168-ff47ad26e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-18 16:44:22 [cuda.py:569] Detected different devices in the system: NVIDIA GeForce RTX 3080, NVIDIA GeForce RTX 5070 Ti. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ALLOWED_LAYER_TYPES' from 'transformers.configuration_utils' (/home/henry/projects/navidrome/musicembed/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, MusicFlamingoConfig\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmusicflamingo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_musicflamingo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     MusicFlamingoEncoder,\n\u001b[32m      8\u001b[39m     MusicFlamingoMultiModalProjector,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m     14\u001b[39m CKPT_DIR = \u001b[33m\"\u001b[39m\u001b[33m./music_flamingo_fp8\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# your local folder\u001b[39;00m\n\u001b[32m     15\u001b[39m AUDIO_COUNT = \u001b[32m1\u001b[39m                    \u001b[38;5;66;03m# one audio item in the prompt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/navidrome/musicembed/.venv/lib/python3.13/site-packages/vllm/__init__.py:74\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m MODULE_ATTRS:\n\u001b[32m     73\u001b[39m     module_name, attr_name = MODULE_ATTRS[name].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     module = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__package__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/navidrome/musicembed/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:20\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeVar\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeam_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     BeamSearchInstance,\n\u001b[32m     16\u001b[39m     BeamSearchOutput,\n\u001b[32m     17\u001b[39m     BeamSearchSequence,\n\u001b[32m     18\u001b[39m     create_sort_beams_key_function,\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     CompilationConfig,\n\u001b[32m     22\u001b[39m     PoolerConfig,\n\u001b[32m     23\u001b[39m     ProfilerConfig,\n\u001b[32m     24\u001b[39m     StructuredOutputsConfig,\n\u001b[32m     25\u001b[39m     is_init_field,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompilation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompilationMode\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     ConvertOption,\n\u001b[32m     30\u001b[39m     HfOverrides,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     TokenizerMode,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/navidrome/musicembed/.venv/lib/python3.13/site-packages/vllm/config/__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoadConfig\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoRAConfig\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     ModelConfig,\n\u001b[32m     20\u001b[39m     iter_architecture_defaults,\n\u001b[32m     21\u001b[39m     try_match_architecture_defaults,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultimodal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiModalConfig\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mobservability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ObservabilityConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/navidrome/musicembed/.venv/lib/python3.13/site-packages/vllm/config/model.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msafetensors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _TYPES \u001b[38;5;28;01mas\u001b[39;00m _SAFETENSORS_TO_TORCH_DTYPE\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ALLOWED_LAYER_TYPES\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattention\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionBackendEnum\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ALLOWED_LAYER_TYPES' from 'transformers.configuration_utils' (/home/henry/projects/navidrome/musicembed/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py)"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "from safetensors.torch import safe_open\n",
    "\n",
    "from transformers import AutoProcessor, MusicFlamingoConfig\n",
    "from transformers.models.musicflamingo.modeling_musicflamingo import (\n",
    "    MusicFlamingoEncoder,\n",
    "    MusicFlamingoMultiModalProjector,\n",
    ")\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "CKPT_DIR = \"./music_flamingo_fp8\"  # your local folder\n",
    "AUDIO_COUNT = 1                    # one audio item in the prompt\n",
    "\n",
    "\n",
    "def load_submodule_weights_from_safetensors(module, ckpt_dir: str, prefix: str):\n",
    "    \"\"\"\n",
    "    Loads weights whose keys start with `prefix` (e.g. \"audio_tower.\") into `module`.\n",
    "    Works with sharded safetensors too.\n",
    "    \"\"\"\n",
    "    sd = {}\n",
    "    st_files = sorted(glob.glob(f\"{ckpt_dir}/*.safetensors\"))\n",
    "    if not st_files:\n",
    "        raise FileNotFoundError(f\"No .safetensors files found in: {ckpt_dir}\")\n",
    "\n",
    "    for path in st_files:\n",
    "        with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for k in f.keys():\n",
    "                if k.startswith(prefix):\n",
    "                    sd[k[len(prefix):]] = f.get_tensor(k)\n",
    "\n",
    "    missing, unexpected = module.load_state_dict(sd, strict=False)\n",
    "    if unexpected:\n",
    "        raise RuntimeError(f\"Unexpected keys when loading {prefix}: {unexpected}\")\n",
    "    # missing can be OK if the module has buffers/extra items; usually small.\n",
    "    return missing\n",
    "\n",
    "\n",
    "# --- 1) Build prompt + audio features using the HF processor ---\n",
    "processor = AutoProcessor.from_pretrained(CKPT_DIR)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Describe this track in full detail - tell me the genre, tempo, and key, \"\n",
    "                        \"then dive into the instruments, production style, and overall mood it creates.\",\n",
    "            },\n",
    "            {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/song_1.mp3\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# prompt string for vLLM (should include exactly 1 audio placeholder for 1 audio item)\n",
    "prompt = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# tokenized+processed tensors for computing audio embeddings in Transformers\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# --- 2) Run ONLY the audio tower + projector in Transformers to create audio_embeds ---\n",
    "cfg = MusicFlamingoConfig.from_pretrained(CKPT_DIR)\n",
    "\n",
    "audio_tower = MusicFlamingoEncoder(cfg).eval().cuda()\n",
    "projector = MusicFlamingoMultiModalProjector(cfg).eval().cuda()\n",
    "\n",
    "load_submodule_weights_from_safetensors(audio_tower, CKPT_DIR, prefix=\"audio_tower.\")\n",
    "load_submodule_weights_from_safetensors(projector,  CKPT_DIR, prefix=\"multi_modal_projector.\")\n",
    "\n",
    "input_features = inputs[\"input_features\"].cuda()\n",
    "input_features_mask = inputs[\"input_features_mask\"].cuda()\n",
    "audio_times = inputs.get(\"audio_times\", None)\n",
    "if audio_times is not None:\n",
    "    audio_times = audio_times.cuda()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # mimic your model.get_audio_features() logic, but keep it 3D for vLLM: (B, T, H)\n",
    "    input_features = input_features.to(dtype=audio_tower.conv1.weight.dtype)\n",
    "    enc_out = audio_tower(input_features, input_features_mask=input_features_mask, audio_times=audio_times)\n",
    "    audio_embeds = projector(enc_out.last_hidden_state)  # (B, Tproj, H)\n",
    "\n",
    "    # same post-length formula as your code\n",
    "    post_lengths = (input_features_mask.sum(-1) - 2) // 2 + 1  # (B,)\n",
    "    # for B=1, slice to valid length\n",
    "    T = int(post_lengths[0].item())\n",
    "    audio_embeds = audio_embeds[:, :T, :]                  # (1, audio_feature_size, H)\n",
    "\n",
    "# vLLM expects CPU tensor is fine; shape must be (num_items, feature_size, hidden_size)\n",
    "audio_embeds_for_vllm = audio_embeds.to(\"cpu\")             # (1, feature_size, H)\n",
    "\n",
    "\n",
    "# --- 3) Run generation in vLLM using the *precomputed* audio embeddings ---\n",
    "# AudioFlamingo3 uses a single placeholder token per audio item (commonly \"<sound>\"). :contentReference[oaicite:2]{index=2}\n",
    "# Your `prompt` from the HF processor should already be in the right format.\n",
    "llm = LLM(\n",
    "    model=CKPT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    limit_mm_per_prompt={\"audio\": AUDIO_COUNT},\n",
    "    enable_mm_embeds=True,  # required for embedding inputs :contentReference[oaicite:3]{index=3}\n",
    "    # max_model_len=...,  # set if you hit context length errors\n",
    ")\n",
    "\n",
    "sampling = SamplingParams(\n",
    "    max_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "outputs = llm.generate(\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\n",
    "            \"audio\": audio_embeds_for_vllm,  # (1, feature_size, hidden)\n",
    "        },\n",
    "    },\n",
    "    sampling_params=sampling,\n",
    ")\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navidrome",
   "language": "python",
   "name": "nav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
