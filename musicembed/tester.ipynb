{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774adf2-137f-48bd-a330-7c6cff9a471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as m\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae9164-234c-4a5a-b6b1-aeff11ae5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [\"/home/henry/projects/navidrome/music/Lorde-Pure_Heroine-24BIT-WEB-FLAC-2013-TVRf/04-lorde-ribs.flac\",\n",
    "        \"/home/henry/projects/navidrome/music/Lorde-Pure_Heroine-24BIT-WEB-FLAC-2013-TVRf/01-lorde-tennis_court.flac\"]\n",
    "prompt = \"Describe this track in full detail - tell me the genre, tempo, and key, then dive into the instruments, production style, and overall mood it creates.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213325e3-fb1d-41f3-96ff-4fd6b076f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = m.MusicFlamingo(\"./music_flamingo_fp8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26cd27-ac1d-4252-8f2e-30b3f193c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = mf.embed_music_from_path(songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e415b-093d-4b49-88d1-981f77a08dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.inference_llm(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5ef6db-ed65-4625-84af-10b3cd664c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mf.prepare_model_input()\n",
    "loaded = mf.load_audio(songs[0])\n",
    "prepared = mf.prepare_music(loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5544ad69-b972-44d8-a9b8-c660df98be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_features\n",
      "torch.Size([9, 128, 3000])\n",
      "input_features_mask\n",
      "torch.Size([9, 3000])\n",
      "audio_times\n",
      "torch.Size([9, 750])\n"
     ]
    }
   ],
   "source": [
    "for i in prepared.keys():\n",
    "    print(i)\n",
    "    print(prepared[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fe5ae0-167a-48c2-8a53-8088576a1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mf.music_processor(\n",
    "    text=text,\n",
    "    audio=None,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0187f4fd-4eaf-4c2d-beb6-3850e2009e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,  10717,   2988,   6469,     78,\n",
       "             11,    264,  79049,  57597,  17847,    369,   4128,    323,   4627,\n",
       "             13,   1913,   1817,   2484,    498,   5258,    458,   7699,  12327,\n",
       "            892,   5610,   4627,    323,  10101,   1467,     11,    498,    686,\n",
       "           5258,    518,   3245,    825,    476,   2176,     26,    990,    697,\n",
       "           1879,   6540,    323,  32711,    311,   1492,    279,   1196,    448,\n",
       "            894,   3383,     13,  72077,    279,  47917,    315,    279,   2213,\n",
       "            894,   1946,   4627,    313,   1580,    567,     75,    724,     82,\n",
       "            315,   3425,    279,   1196,   6738,    432,   7699,     11,   4627,\n",
       "             11,    476,   5112,     13, 151645,    198, 151644,    872,    198,\n",
       "         151667,  74785,    419,   3754,    304,   2480,   7716,    481,   3291,\n",
       "            752,    279,  17328,     11,  23230,     11,    323,   1376,     11,\n",
       "           1221,  29863,   1119,    279,  23316,     11,   5670,   1707,     11,\n",
       "            323,   8084,  19671,    432,  11450,     13, 151645,    198, 151644,\n",
       "          77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d503bd0d-7a8a-41ea-a8eb-7eadd031d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = mf.load_audio(songs[0])\n",
    "inputs = mf.prepare_music(loaded)\n",
    "embedded = mf.embed_music(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7278b4-3461-4ed7-8b73-1f295cdf662d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-11.5000,  -9.5625,  -2.9375,  ...,   6.6250,   2.1406,   3.7344],\n",
       "        [ -4.2500,  -3.5000,   1.2031,  ...,   3.5469,   0.3809,   3.8281],\n",
       "        [  0.1084,  -3.3281,   1.4766,  ...,   2.5469,   0.7344,  -0.6836],\n",
       "        ...,\n",
       "        [ -1.2578,  -1.5156,  -0.2451,  ...,  -0.2754,  -1.8281,   0.3398],\n",
       "        [ -2.1719,  -0.1943,  -0.0674,  ...,  -0.2295,  -0.8281,  -1.1719],\n",
       "        [ -1.0859,   1.2500,  -0.8125,  ...,   0.9023,   0.1836,  -1.7734]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fccf36-8e90-4001-b18a-cea4456b7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-21 01:58:02] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.\n",
      "[2026-01-21 01:58:02] INFO model_config.py:913: Downcasting torch.float32 to torch.float16.\n",
      "[2026-01-21 01:58:02] INFO model_runner.py:613: Init torch distributed begin.\n",
      "[2026-01-21 01:58:02] INFO model_runner.py:722: Init torch distributed ends. mem usage=0.00 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-21 01:58:02] WARNING registry.py:103: Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined\n",
      "[2026-01-21 01:58:02] INFO model_runner.py:729: Load weight begin. avail mem=13.85 GB\n",
      "[2026-01-21 01:58:02] INFO common.py:2772: Detected fp8 checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab63d24e6dd4bce9a26c8b3c783e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-21 01:58:04] INFO model_runner.py:851: Load weight end. type=MusicFlamingoQwen2ForCausalLM, dtype=torch.float16, avail mem=6.65 GB, mem usage=7.20 GB.\n",
      "[2026-01-21 01:58:04] INFO common.py:2772: Using KV cache dtype: torch.float16\n",
      "[2026-01-21 01:58:04] INFO memory_pool.py:476: KV Cache is allocated. #tokens: 30119, K size: 0.80 GB, V size: 0.80 GB\n",
      "[2026-01-21 01:58:04] INFO model_runner.py:2037: Memory pool end. avail mem=4.88 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_total_num_tokens=30119\n"
     ]
    }
   ],
   "source": [
    "runner = m._get_sglang_runner(\"./music_flamingo_fp8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9de24a7-2fd1-4753-be9e-bfe8fbcb3b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ModelRunner in module sglang.srt.model_executor.model_runner object:\n",
      "\n",
      "class ModelRunner(builtins.object)\n",
      " |  ModelRunner(\n",
      " |      model_config: sglang.srt.configs.model_config.ModelConfig,\n",
      " |      mem_fraction_static: float,\n",
      " |      gpu_id: int,\n",
      " |      tp_rank: int,\n",
      " |      tp_size: int,\n",
      " |      moe_ep_rank: int,\n",
      " |      moe_ep_size: int,\n",
      " |      pp_rank: int,\n",
      " |      pp_size: int,\n",
      " |      nccl_port: int,\n",
      " |      server_args: sglang.srt.server_args.ServerArgs,\n",
      " |      dp_rank: Optional[int] = None,\n",
      " |      is_draft_worker: bool = False,\n",
      " |      req_to_token_pool: Optional[sglang.srt.mem_cache.memory_pool.ReqToTokenPool] = None,\n",
      " |      token_to_kv_pool_allocator: Optional[sglang.srt.mem_cache.allocator.BaseTokenToKVPoolAllocator] = None\n",
      " |  )\n",
      " |\n",
      " |  ModelRunner runs the forward passes of the models.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      model_config: sglang.srt.configs.model_config.ModelConfig,\n",
      " |      mem_fraction_static: float,\n",
      " |      gpu_id: int,\n",
      " |      tp_rank: int,\n",
      " |      tp_size: int,\n",
      " |      moe_ep_rank: int,\n",
      " |      moe_ep_size: int,\n",
      " |      pp_rank: int,\n",
      " |      pp_size: int,\n",
      " |      nccl_port: int,\n",
      " |      server_args: sglang.srt.server_args.ServerArgs,\n",
      " |      dp_rank: Optional[int] = None,\n",
      " |      is_draft_worker: bool = False,\n",
      " |      req_to_token_pool: Optional[sglang.srt.mem_cache.memory_pool.ReqToTokenPool] = None,\n",
      " |      token_to_kv_pool_allocator: Optional[sglang.srt.mem_cache.allocator.BaseTokenToKVPoolAllocator] = None\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  apply_torch_tp(self)\n",
      " |\n",
      " |  can_run_piecewise_cuda_graph(self)\n",
      " |\n",
      " |  check_quantized_moe_compatibility(self)\n",
      " |\n",
      " |  check_weights(self, action: str)\n",
      " |\n",
      " |  compute_logprobs_only(\n",
      " |      self,\n",
      " |      logits_output: sglang.srt.layers.logits_processor.LogitsProcessorOutput,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch\n",
      " |  ) -> None\n",
      " |      Compute token_ids_logprobs without performing sampling.\n",
      " |\n",
      " |      Optimized path for prefill-only requests that need token_ids_logprobs but don't\n",
      " |      require next token generation. Skips expensive sampling operations\n",
      " |      while still providing requested probability information.\n",
      " |\n",
      " |      Args:\n",
      " |          logits_output: The logits output from the model forward\n",
      " |          forward_batch: The forward batch that generates logits_output\n",
      " |\n",
      " |  destroy_weights_update_group(self, group_name)\n",
      " |\n",
      " |  forward(\n",
      " |      self,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch,\n",
      " |      skip_attn_backend_init: bool = False,\n",
      " |      pp_proxy_tensors: Optional[sglang.srt.model_executor.forward_batch_info.PPProxyTensors] = None,\n",
      " |      reinit_attn_backend: bool = False,\n",
      " |      split_forward_count: int = 1\n",
      " |  ) -> Tuple[Union[sglang.srt.layers.logits_processor.LogitsProcessorOutput, sglang.srt.model_executor.forward_batch_info.PPProxyTensors], bool]\n",
      " |\n",
      " |  forward_decode(\n",
      " |      self,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch,\n",
      " |      skip_attn_backend_init: bool = False,\n",
      " |      pp_proxy_tensors=None\n",
      " |  ) -> Union[sglang.srt.layers.logits_processor.LogitsProcessorOutput, sglang.srt.model_executor.forward_batch_info.PPProxyTensors]\n",
      " |\n",
      " |  forward_extend(\n",
      " |      self,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch,\n",
      " |      skip_attn_backend_init: bool = False,\n",
      " |      pp_proxy_tensors=None\n",
      " |  ) -> Union[sglang.srt.layers.logits_processor.LogitsProcessorOutput, sglang.srt.model_executor.forward_batch_info.PPProxyTensors, sglang.srt.layers.pooler.EmbeddingPoolerOutput]\n",
      " |\n",
      " |  forward_idle(\n",
      " |      self,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch,\n",
      " |      pp_proxy_tensors=None\n",
      " |  ) -> Union[sglang.srt.layers.logits_processor.LogitsProcessorOutput, sglang.srt.model_executor.forward_batch_info.PPProxyTensors]\n",
      " |\n",
      " |  forward_split_prefill(\n",
      " |      self,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch,\n",
      " |      reinit_attn_backend: bool = False,\n",
      " |      forward_count: int = 1\n",
      " |  ) -> sglang.srt.layers.logits_processor.LogitsProcessorOutput\n",
      " |\n",
      " |  get_weights_by_name(self, name: str, truncate_size: int = 100) -> Optional[torch.Tensor]\n",
      " |      Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.\n",
      " |\n",
      " |      Only used for unit test with an unoptimized performance.\n",
      " |      For optimized performance, please use torch.save and torch.load.\n",
      " |\n",
      " |  handle_max_mamba_cache(self, total_rest_memory)\n",
      " |\n",
      " |  init_attention_backend(self)\n",
      " |      Init attention kernel backend.\n",
      " |\n",
      " |  init_cublas(self)\n",
      " |      We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\n",
      " |\n",
      " |  init_device_graphs(self)\n",
      " |      Capture device graphs.\n",
      " |\n",
      " |  init_double_sparsity_channel_config(self, selected_channel)\n",
      " |\n",
      " |  init_lora_manager(self)\n",
      " |\n",
      " |  init_memory_pool(\n",
      " |      self,\n",
      " |      total_gpu_memory: int,\n",
      " |      max_num_reqs: Optional[int] = None,\n",
      " |      max_total_tokens: Optional[int] = None\n",
      " |  )\n",
      " |\n",
      " |  init_mindspore_runner(self)\n",
      " |\n",
      " |  init_piecewise_cuda_graphs(self)\n",
      " |      Initialize piecewise CUDA graph runner.\n",
      " |\n",
      " |  init_threads_binding(self)\n",
      " |\n",
      " |  init_torch_distributed(self)\n",
      " |\n",
      " |  init_weights_send_group_for_remote_instance(\n",
      " |      self,\n",
      " |      master_address,\n",
      " |      ports,\n",
      " |      group_rank,\n",
      " |      world_size,\n",
      " |      group_name,\n",
      " |      backend='nccl'\n",
      " |  )\n",
      " |\n",
      " |  init_weights_update_group(\n",
      " |      self,\n",
      " |      master_address,\n",
      " |      master_port,\n",
      " |      rank_offset,\n",
      " |      world_size,\n",
      " |      group_name,\n",
      " |      backend='nccl'\n",
      " |  )\n",
      " |      Initialize the Torch process group for model parameter updates.\n",
      " |\n",
      " |      `_model_update_group` is used in the RLHF workflow, where rank\n",
      " |      0 is the actor model in the training engine, and the other ranks are\n",
      " |      the inference engine, which is used for rollout.\n",
      " |\n",
      " |      In the RLHF workflow, the training engine updates the model\n",
      " |      weights/parameters online, and broadcasts them to the inference\n",
      " |      engine through the `_model_update_group` process group.\n",
      " |\n",
      " |  initialize(self, min_per_gpu_memory: float)\n",
      " |\n",
      " |  kernel_warmup(self)\n",
      " |      Warmup and tune kernels before cuda graph capture.\n",
      " |      Currently only doing FlashInfer autotune.\n",
      " |\n",
      " |  load_lora_adapter(self, lora_ref: sglang.srt.lora.lora_registry.LoRARef)\n",
      " |      Load a new lora adapter from disk or huggingface.\n",
      " |\n",
      " |  load_model(self)\n",
      " |\n",
      " |  model_specific_adjustment(self)\n",
      " |\n",
      " |  profile_max_num_token(self, total_gpu_memory: int)\n",
      " |\n",
      " |  sample(\n",
      " |      self,\n",
      " |      logits_output: sglang.srt.layers.logits_processor.LogitsProcessorOutput,\n",
      " |      forward_batch: sglang.srt.model_executor.forward_batch_info.ForwardBatch\n",
      " |  ) -> torch.Tensor\n",
      " |      Sample and compute logprobs and update logits_output.\n",
      " |\n",
      " |      Args:\n",
      " |          logits_output: The logits output from the model forward\n",
      " |          forward_batch: The forward batch that generates logits_output\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of next_token_ids\n",
      " |\n",
      " |  save_remote_model(self, url: str)\n",
      " |\n",
      " |  save_sharded_model(\n",
      " |      self,\n",
      " |      path: str,\n",
      " |      pattern: Optional[str] = None,\n",
      " |      max_size: Optional[int] = None\n",
      " |  )\n",
      " |\n",
      " |  send_weights_to_remote_instance(self, master_address, ports, group_name)\n",
      " |\n",
      " |  set_num_token_hybrid(self)\n",
      " |\n",
      " |  unload_lora_adapter(self, lora_ref: sglang.srt.lora.lora_registry.LoRARef)\n",
      " |      Unload a lora adapter that was previously loaded during initialization or dynamic loading.\n",
      " |\n",
      " |  update_decode_attn_backend(self, stream_idx: int)\n",
      " |\n",
      " |  update_expert_location(\n",
      " |      self,\n",
      " |      new_expert_location_metadata: sglang.srt.eplb.expert_location.ExpertLocationMetadata,\n",
      " |      update_layer_ids: List[int]\n",
      " |  )\n",
      " |\n",
      " |  update_weights_from_disk(\n",
      " |      self,\n",
      " |      model_path: str,\n",
      " |      load_format: str,\n",
      " |      weight_name_filter: Optional[Callable[[str], bool]] = None,\n",
      " |      recapture_cuda_graph: bool = False\n",
      " |  ) -> tuple[bool, str]\n",
      " |      Update engine weights in-place from the disk.\n",
      " |\n",
      " |  update_weights_from_distributed(\n",
      " |      self,\n",
      " |      names,\n",
      " |      dtypes,\n",
      " |      shapes,\n",
      " |      group_name,\n",
      " |      load_format: Optional[str] = None\n",
      " |  )\n",
      " |      Update specific parameter in the model weights online\n",
      " |      through `_model_update_group` process group.\n",
      " |\n",
      " |      Args:\n",
      " |          name: the name of the parameter to be updated.\n",
      " |          dtype: the data type of the parameter to be updated.\n",
      " |          shape: the shape of the parameter to be updated.\n",
      " |\n",
      " |  update_weights_from_ipc(self, recv_req)\n",
      " |      Update weights from IPC for checkpoint-engine integration.\n",
      " |\n",
      " |  update_weights_from_tensor(\n",
      " |      self,\n",
      " |      named_tensors: List[Tuple[str, Union[torch.Tensor, ForwardRef('LocalSerializedTensor')]]],\n",
      " |      load_format: Optional[str] = None\n",
      " |  )\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  hybrid_gdn_config\n",
      " |\n",
      " |  kimi_linear_config\n",
      " |\n",
      " |  mamba2_config\n",
      " |\n",
      " |  mambaish_config\n",
      " |\n",
      " |  model_is_mrope\n",
      " |      Detect if the model has \"mrope\" rope_scaling type.\n",
      " |      mrope requires keep \"rope_deltas\" between prompt and decoding phases.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbe072-dda1-442d-a726-4919b30c5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6cc40e-ec61-40a1-be24-73256716b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio1 = m.load_audio(songs[0])\n",
    "audio2 = m.load_audio(songs[1])\n",
    "# ready to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92d8735-ac5a-45ac-9164-c045a5eeaf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5def1e2474f244fcb7cafe4d3a5b2156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_embedder = m.load_audio_embedder(\"./music_flamingo_fp8\", device_map=\"cuda\").eval()\n",
    "processor = m.load_music_flamingo_processor(\"./music_flamingo_fp8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d74c08e-fab1-4322-9103-c8a82b58de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can take a list of songs\n",
    "inputs = processor(\n",
    "    audio=audio1,\n",
    "    return_tensors=\"pt\",\n",
    ").to(audio_embedder.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ee10fc-f7e2-41fb-acd4-07fdaac054c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 ms ± 3.61 ms per loop (mean ± std. dev. of 7 runs, 25 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 25\n",
    "with torch.inference_mode():\n",
    "    embeddings = audio_embedder.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3d312b-0276-4f1f-b0ef-2cd961fa8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embedder = torch.compile(audio_embedder, fullgraph=True, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4771414e-cffa-47b5-b530-837f0b0bf37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 ms ± 122 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.inference_mode():\n",
    "    embeddings = audio_embedder.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76966ceb-9080-468c-a0c6-f6677c380a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 ms ± 110 μs per loop (mean ± std. dev. of 7 runs, 25 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 25\n",
    "with torch.inference_mode():\n",
    "    embeddings = audio_embedder.get_audio_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa3e06a-00a8-483f-bbd0-d97e34bca7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"audio\", \"path\": songs[0]},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Build prompt without loading audio inside apply_chat_template\n",
    "prompt = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    audio=\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navidrome",
   "language": "python",
   "name": "nav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
